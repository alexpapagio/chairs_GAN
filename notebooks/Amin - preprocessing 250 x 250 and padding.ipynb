{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a0ad9d",
   "metadata": {},
   "source": [
    "### EDA on kaggle dataset of chairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57132799",
   "metadata": {},
   "source": [
    "# Importing Data into DF and viewing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6441c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 10:37:28.459244: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-23 10:37:28.459337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-23 10:37:28.521855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-23 10:37:28.650595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 10:37:30.441043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62466183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT\n",
    "\n",
    "# chair_folder = 'Yolo_Images'\n",
    "\n",
    "# # Function to read and display images\n",
    "# def display_images(folder_path):\n",
    "#     images = os.listdir(folder_path)\n",
    "#     image_list = []\n",
    "    \n",
    "#     for image_name in images:\n",
    "#         if image_name.endswith('.jpg'):\n",
    "#             image_path = os.path.join(folder_path, image_name)\n",
    "            \n",
    "#             # Read image using PIL (Python Imaging Library)\n",
    "#             image = Image.open(image_path)\n",
    "            \n",
    "#             # Convert image to numpy array\n",
    "#             image_data = plt.imread(image_path)\n",
    "            \n",
    "#             # Append image and file name to a list\n",
    "#             image_list.append({'Image': image_data, 'File Name': image_name})\n",
    "    \n",
    "#     # Create a DataFrame from the image data\n",
    "#     df_images = pd.DataFrame(image_list)\n",
    "    \n",
    "#     return df_images\n",
    "\n",
    "# # Create DataFrame with images\n",
    "# df_images = display_images(chair_folder)\n",
    "# df_images['File Name'] = (df_images.index + 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bc2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_images.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a1a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Display the first image\n",
    "# first_image = df_images.iloc[2]['Image']\n",
    "# first_image_name = df_images.iloc[2]['File Name']\n",
    "\n",
    "# plt.imshow(first_image)\n",
    "# plt.title(first_image_name)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821e489",
   "metadata": {},
   "source": [
    "## Resolution of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de1d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UNCOMMENT\n",
    "# # Function to get image resolution\n",
    "# def get_image_resolution(image_array):\n",
    "#     if len(image_array.shape) == 3:  # Check if it's a color image\n",
    "#         height, width, _ = image_array.shape\n",
    "#     else:  # Grayscale or single-channel image\n",
    "#         height, width = image_array.shape\n",
    "#     return (width, height)\n",
    "\n",
    "# # Apply the function to get resolution for the first image\n",
    "# first_image_resolution = get_image_resolution(df_images['Image'][0])\n",
    "\n",
    "# # Print the resolution of the first image\n",
    "# print(\"Resolution of the first image:\", first_image_resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd9b00",
   "metadata": {},
   "source": [
    "## Each Image has a different resolution, all pictures must be converted to 250 x 250 pixels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433ee237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing to change resolution of dataset into 100 x 100 resolution and normalize -- Greyscale conversion \n",
    "\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Function to resize and normalize an image\n",
    "# def preprocess_image(image_path, target_size=(250, 250)):\n",
    "#     # Open the image\n",
    "#     with Image.open(image_path) as img:\n",
    "#         # Resize the image to the target size\n",
    "#         resized_img = img.resize(target_size)\n",
    "#         # Convert the image to grayscale and convert to numpy array\n",
    "#         resized_array = np.array(resized_img.convert('RGB'))\n",
    "#         # Normalize pixel values to be between 0 and 1\n",
    "#         normalized_array = resized_array / 255.0\n",
    "#     return normalized_array\n",
    "\n",
    "# # Path to the folder containing your images\n",
    "# images_folder = Path('train/chair')\n",
    "\n",
    "# # List to store processed images\n",
    "# processed_images = []\n",
    "\n",
    "# # Loop through each image file in the folder\n",
    "# for image_file in images_folder.glob(\"*.jpg\"):\n",
    "#     processed_image = preprocess_image(image_file)\n",
    "#     processed_images.append(processed_image)\n",
    "\n",
    "# # Convert the list of images to a numpy array\n",
    "# processed_images_array = np.array(processed_images)\n",
    "\n",
    "\n",
    "# # Check the shape of the processed images array\n",
    "# print(\"Processed Images Shape:\", processed_images_array.shape)\n",
    "\n",
    "# # Display one of the processed images\n",
    "# plt.imshow(processed_images_array[1510], cmap='gray')\n",
    "# plt.title(\"Processed Image\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d3bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc33454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ae745d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing to change resolution of dataset into 250 x 250 resolution and normalize -- RGB\n",
    "\n",
    "# # Takes all image types and converts to .jpg, skip files that are not pictures in the image directory\n",
    "# # Preprocess into 250 x 250 and makes all pictures have the same aspect.\n",
    "# # Normalizes each image and stores in a norm_processed_images_array for feature embeddings.\n",
    "# # Stores all the newly processed .jpgs in a new folder.\n",
    "\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# import mimetypes\n",
    "\n",
    "# def preprocess_image(image_path, target_size=(250, 250), fill_color=(255, 255, 255)):\n",
    "#     \"\"\"\n",
    "#     Preprocess an image by resizing it to the target size while maintaining aspect ratio.\n",
    "#     The image will be resized to fit within the target size and padded with fill color if necessary.\n",
    "    \n",
    "#     Args:\n",
    "#         image_path (str): Path to the image file.\n",
    "#         target_size (tuple): Target size to resize the image to.\n",
    "#         fill_color (tuple): RGB color tuple for fill color when padding.\n",
    "    \n",
    "#     Returns:\n",
    "#         numpy.ndarray: Processed image array.\n",
    "#     \"\"\"\n",
    "#     image = Image.open(image_path)\n",
    "#     width, height = image.size\n",
    "#     target_width, target_height = target_size\n",
    "    \n",
    "#     # Calculate aspect ratios\n",
    "#     aspect_ratio = width / height\n",
    "#     target_aspect_ratio = target_width / target_height\n",
    "    \n",
    "#     # Resize image to fit within the target size\n",
    "#     if aspect_ratio > target_aspect_ratio:\n",
    "#         # Fit width to target width\n",
    "#         new_width = target_width\n",
    "#         new_height = int(target_width / aspect_ratio)\n",
    "#     else:\n",
    "#         # Fit height to target height\n",
    "#         new_height = target_height\n",
    "#         new_width = int(target_height * aspect_ratio)\n",
    "    \n",
    "#     # Resize image\n",
    "#     resized_img = image.resize((new_width, new_height))\n",
    "    \n",
    "#     # Create new image with target size and fill color\n",
    "#     padded_img = Image.new('RGB', target_size, fill_color)\n",
    "    \n",
    "#     # Paste resized image onto the new image\n",
    "#     left = (target_width - new_width) // 2\n",
    "#     top = (target_height - new_height) // 2\n",
    "#     padded_img.paste(resized_img, (left, top))\n",
    "    \n",
    "#     # Convert to numpy array\n",
    "#     resized_array = np.array(padded_img)\n",
    "    \n",
    "#     return resized_array\n",
    "\n",
    "# def normalize_images(images_array):\n",
    "#     \"\"\"\n",
    "#     Normalize images array between 0 and 1.\n",
    "    \n",
    "#     Args:\n",
    "#         images_array (numpy.ndarray): Array of images.\n",
    "    \n",
    "#     Returns:\n",
    "#         numpy.ndarray: Normalized images array.\n",
    "#     \"\"\"\n",
    "#     return images_array / 255.0\n",
    "\n",
    "# # Path to the folder containing images\n",
    "# images_folder = Path('Yolo_Images')\n",
    "\n",
    "# # Path to the folder to save processed images\n",
    "# processed_folder = Path('processed_images')\n",
    "\n",
    "# # Create the processed images folder if it doesn't exist\n",
    "# processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # List to store processed images\n",
    "# processed_images = []\n",
    "\n",
    "# # Iterate through images in the folder\n",
    "# for image_file in images_folder.glob(\"*.*\"):\n",
    "#     # Check if the file is an image\n",
    "#     if mimetypes.guess_type(image_file)[0] is not None and \"image\" in mimetypes.guess_type(image_file)[0]:\n",
    "#         # Preprocess the image\n",
    "#         processed_image = preprocess_image(image_file)\n",
    "        \n",
    "#         # Append the processed image to the list\n",
    "#         processed_images.append(processed_image)\n",
    "        \n",
    "#         # Save the processed image\n",
    "#         image_name = image_file.stem + \".jpg\"\n",
    "#         save_path = processed_folder / image_name\n",
    "#         Image.fromarray(processed_image).save(save_path)\n",
    "\n",
    "# # Convert the list of images to a numpy array\n",
    "# processed_images_array = np.array(processed_images)\n",
    "\n",
    "# # Normalize the processed images between 0 and 1\n",
    "# norm_processed_images_array = normalize_images(processed_images_array)\n",
    "\n",
    "# # Check the shape of the processed images array\n",
    "# print(\"Processed Images Shape:\", processed_images_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277bc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(norm_processed_images_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c3fedf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Display one of the processed images\n",
    "# plt.imshow(norm_processed_images_array[19], cmap='gray')\n",
    "# plt.title(\"Processed Image\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb053bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aedd12a1",
   "metadata": {},
   "source": [
    "## Using Yolo v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318bc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75194de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Create a new YOLO model from scratch\n",
    "# model = YOLO('yolov8n.yaml')\n",
    "\n",
    "# # Load a pretrained YOLO model (recommended for training)\n",
    "# model = YOLO('yolov8n.pt')\n",
    "\n",
    "# # Train the model using the 'coco128.yaml' dataset for 3 epochs\n",
    "# results = model.train(data='coco128.yaml', epochs=3)\n",
    "\n",
    "# # Evaluate the model's performance on the validation set\n",
    "# results = model.val()\n",
    "\n",
    "# # Perform object detection on an image using the model\n",
    "# results = model(first_image)\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# success = model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec91416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "358edfaa",
   "metadata": {},
   "source": [
    "## interactive representations of the latent space with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d8b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (0.5.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from umap-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from umap-learn) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from umap-learn) (1.3.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from umap-learn) (0.59.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from umap-learn) (0.5.11)\n",
      "Requirement already satisfied: tqdm in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from umap-learn) (4.64.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.42.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc994e",
   "metadata": {},
   "source": [
    "## Using ResNet the UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a17aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: filelock in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (2.8.7)\n",
      "Requirement already satisfied: jinja2 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (2022.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
      "Requirement already satisfied: numpy in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7c6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a873de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b875f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4fa9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torchvision.models import resnet50\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from umap import UMAP\n",
    "# import plotly.express as px\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Function to preprocess images for ResNet\n",
    "# def preprocess_resnet(image):\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "#     return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# # Load ResNet model\n",
    "# resnet = resnet50(pretrained=True)\n",
    "# resnet.eval()\n",
    "\n",
    "# # Function to extract ResNet features\n",
    "# def extract_features(image_tensor):\n",
    "#     with torch.no_grad():\n",
    "#         features = resnet(image_tensor)\n",
    "#     return features[0]  # Get the feature vector, remove the batch dimension\n",
    "\n",
    "# # Path to the folder containing images\n",
    "# images_folder = Path('processed_images')\n",
    "\n",
    "# # List to store ResNet features and image paths\n",
    "# all_features = []\n",
    "# image_paths = []\n",
    "\n",
    "# # Process the first 500 images\n",
    "# num_images = 500\n",
    "# batch_size = 50\n",
    "\n",
    "# for i, image_file in enumerate(images_folder.glob(\"*.jpg\")):\n",
    "#     if i >= num_images:\n",
    "#         break\n",
    "#     image_paths.append(image_file.name)\n",
    "#     image = Image.open(image_file)\n",
    "#     image_tensor = preprocess_resnet(image)\n",
    "#     features = extract_features(image_tensor)\n",
    "#     all_features.append(features.numpy())  # Convert to numpy array and append\n",
    "\n",
    "# all_features = np.array(all_features)\n",
    "# image_paths = np.array(image_paths)\n",
    "\n",
    "# # Compute a feature score for each image (using an example of a feature dimension)\n",
    "# # There are a maximum of 1000 dimensions which can be modelled.\n",
    "# feature_dim = 999\n",
    "# feature_scores = all_features[:, feature_dim]  # Assuming the feature dimension is 2048\n",
    "\n",
    "# # Normalize the feature scores to range between 0 and 1\n",
    "# scaler = MinMaxScaler()\n",
    "# feature_scores_normalized = scaler.fit_transform(feature_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# # Map the normalized scores to a colormap\n",
    "# cmap = plt.get_cmap('viridis')  # You can change 'viridis' to any other colormap\n",
    "# colors = [cmap(score) for score in feature_scores_normalized]\n",
    "\n",
    "# # Reduce dimensions using UMAP\n",
    "# reducer = UMAP(n_components=3)  # Set n_components to 3 for 3D visualization\n",
    "# embedding = reducer.fit_transform(all_features)\n",
    "\n",
    "# # Create DataFrame for the reduced dimensions\n",
    "# umap_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "\n",
    "# # Add image paths and feature scores to the DataFrame\n",
    "# umap_df['image_path'] = image_paths\n",
    "# umap_df['feature_score'] = feature_scores_normalized\n",
    "\n",
    "# # Create an interactive 3D plot with plotly express\n",
    "# fig = px.scatter_3d(umap_df, x='UMAP1', y='UMAP2', z='UMAP3', color='feature_score',\n",
    "#                     title=\"UMAP Visualization with Feature Scores in 3D\",\n",
    "#                     color_continuous_scale='viridis', hover_data=['image_path', 'feature_score'])\n",
    "# fig.update_traces(marker=dict(size=5, opacity=0.8, color=colors))\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15c115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34ce9abc",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614c15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torchvision.models import resnet50\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Function to preprocess images for ResNet\n",
    "# def preprocess_resnet(image):\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "#     return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# # Load ResNet model\n",
    "# resnet = resnet50(pretrained=True)\n",
    "# resnet.eval()\n",
    "\n",
    "# # Function to extract ResNet features\n",
    "# def extract_features(image_tensor):\n",
    "#     with torch.no_grad():\n",
    "#         features = resnet(image_tensor)\n",
    "#     return features.squeeze().numpy()\n",
    "\n",
    "# # Path to the folder containing preprocessed chair images\n",
    "# images_folder = Path('processed_images')\n",
    "\n",
    "# # List to store ResNet features and image paths\n",
    "# all_features = []\n",
    "# image_paths = []\n",
    "\n",
    "# # Process all chair images\n",
    "# for image_file in images_folder.glob(\"*.jpg\"):\n",
    "#     image_paths.append(image_file.name)\n",
    "#     image = Image.open(image_file)\n",
    "#     image_tensor = preprocess_resnet(image)\n",
    "#     features = extract_features(image_tensor)\n",
    "#     all_features.append(features)\n",
    "\n",
    "# all_features = np.array(all_features)\n",
    "# image_paths = np.array(image_paths)\n",
    "\n",
    "# # Create a DataFrame to store the embeddings and image paths\n",
    "# df = pd.DataFrame(all_features)\n",
    "# df['image_path'] = image_paths\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv('chair_embeddings_with_metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d54c9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT\n",
    "\n",
    "# # Function to preprocess images for ResNet\n",
    "# def preprocess_resnet(image):\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "#     return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# # Load ResNet model\n",
    "# resnet = resnet50(pretrained=True)\n",
    "# resnet.eval()\n",
    "\n",
    "# # Function to extract ResNet features\n",
    "# def extract_features(image_tensor):\n",
    "#     with torch.no_grad():\n",
    "#         features = resnet(image_tensor)\n",
    "#     return features[0]  # Get the feature vector, remove the batch dimension\n",
    "\n",
    "# # Path to the folder containing images\n",
    "# images_folder = Path('processed_images')\n",
    "\n",
    "# # List to store ResNet features and image paths\n",
    "# all_features = []\n",
    "# image_paths = []\n",
    "\n",
    "# # Process the first 500 images\n",
    "# num_images = 500\n",
    "# batch_size = 50\n",
    "\n",
    "# for i, image_file in enumerate(images_folder.glob(\"*.jpg\")):\n",
    "#     if i >= num_images:\n",
    "#         break\n",
    "#     image_paths.append(image_file.name)\n",
    "#     image = Image.open(image_file)\n",
    "#     image_tensor = preprocess_resnet(image)\n",
    "#     features = extract_features(image_tensor)\n",
    "#     all_features.append(features.numpy())  # Convert to numpy array and append\n",
    "\n",
    "# all_features = np.array(all_features)\n",
    "# image_paths = np.array(image_paths)\n",
    "\n",
    "# # Compute a feature score for each image (using an example of a feature dimension)\n",
    "# # There are a maximum of 1000 dimensions which can be modelled.\n",
    "# feature_dim = 999\n",
    "# feature_scores = all_features[:, feature_dim]  # Assuming the feature dimension is 2048\n",
    "\n",
    "# # Normalize the feature scores to range between 0 and 1\n",
    "# scaler = MinMaxScaler()\n",
    "# feature_scores_normalized = scaler.fit_transform(feature_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# # Map the normalized scores to a colormap\n",
    "# cmap = plt.get_cmap('viridis')  # You can change 'viridis' to any other colormap\n",
    "# colors = [cmap(score) for score in feature_scores_normalized]\n",
    "\n",
    "# # Reduce dimensions using UMAP\n",
    "# reducer = UMAP(n_components=3)  # Set n_components to 3 for 3D visualization\n",
    "# embedding = reducer.fit_transform(all_features)\n",
    "\n",
    "# # Create DataFrame for the reduced dimensions\n",
    "# umap_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "\n",
    "# # Add image paths and feature scores to the DataFrame\n",
    "# umap_df['image_path'] = image_paths\n",
    "# umap_df['feature_score'] = feature_scores_normalized\n",
    "\n",
    "# # Create an interactive 3D plot with plotly express\n",
    "# fig = px.scatter_3d(umap_df, x='UMAP1', y='UMAP2', z='UMAP3', color='feature_score',\n",
    "#                     title=\"UMAP Visualization with Feature Scores in 3D\",\n",
    "#                     color_continuous_scale='viridis', hover_data=['image_path', 'feature_score'])\n",
    "# fig.update_traces(marker=dict(size=5, opacity=0.8, color=colors))\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d360471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT\n",
    "# # Function to display image on a plot\n",
    "# def display_image(image_data, ax):\n",
    "#     ax.imshow(image_data)\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # Function to update the plot\n",
    "# def update_plot(b):\n",
    "#     # Find the images in the DataFrame based on the file names entered in text_box1 and text_box2\n",
    "#     selected_image1 = df_images[df_images['File Name'] == text_box1.value]['Image'].values\n",
    "#     selected_image2 = df_images[df_images['File Name'] == text_box2.value]['Image'].values\n",
    "    \n",
    "#     if len(selected_image1) > 0 and len(selected_image2) > 0:\n",
    "#         ax1.clear()\n",
    "#         ax2.clear()\n",
    "        \n",
    "#         display_image(selected_image1[0], ax1)\n",
    "#         ax1.set_title(text_box1.value, fontsize=10)\n",
    "        \n",
    "#         display_image(selected_image2[0], ax2)\n",
    "#         ax2.set_title(text_box2.value, fontsize=10)\n",
    "        \n",
    "#         fig.canvas.draw()\n",
    "#         with out:\n",
    "#             clear_output(wait=True)\n",
    "#             display(fig)\n",
    "#     else:\n",
    "#         with out:\n",
    "#             clear_output(wait=True)\n",
    "#             print('One or more images not found.')\n",
    "\n",
    "# # Create initial empty plots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# # Create initial empty images\n",
    "# ax1.axis('off')\n",
    "# ax2.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Create text boxes for image paths\n",
    "# text_box1 = widgets.Text(description='Image 1:', value='', continuous_update=False)\n",
    "# text_box2 = widgets.Text(description='Image 2:', value='', continuous_update=False)\n",
    "\n",
    "# # Create update button\n",
    "# button = widgets.Button(description='Update Plots')\n",
    "\n",
    "# # Create output widget\n",
    "# out = widgets.Output()\n",
    "\n",
    "# # Display widgets\n",
    "# display(widgets.HBox([text_box1, text_box2, button]))\n",
    "# display(out)\n",
    "\n",
    "# # Define button click event handler\n",
    "# button.on_click(update_plot)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274e2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d506a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a42e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574f814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f183b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9fe3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9062a00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with updated 'File Name':\n",
      "<bound method NDFrame.head of                                                   Image File Name\n",
      "0     [[[255, 255, 255], [255, 255, 255], [255, 255,...  4581.jpg\n",
      "1     [[[255, 255, 255], [255, 255, 255], [255, 255,...  4009.jpg\n",
      "2     [[[195, 196, 201], [195, 196, 201], [195, 196,...  1587.jpg\n",
      "3     [[[205, 191, 190], [205, 191, 188], [205, 191,...  3235.jpg\n",
      "4     [[[255, 255, 255], [255, 255, 255], [255, 255,...  1727.jpg\n",
      "...                                                 ...       ...\n",
      "5647  [[[226, 226, 226], [226, 226, 226], [227, 227,...  1784.jpg\n",
      "5648  [[[255, 255, 255], [255, 255, 255], [255, 255,...  4957.jpg\n",
      "5649  [[[213, 212, 210], [213, 212, 210], [213, 212,...  3761.jpg\n",
      "5650  [[[248, 247, 252], [248, 247, 252], [248, 247,...  4228.jpg\n",
      "5651  [[[212, 210, 224], [212, 210, 224], [212, 210,...  1760.jpg\n",
      "\n",
      "[5652 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "chair_folder = 'Yolo_Images'\n",
    "\n",
    "# Function to read and display images\n",
    "def display_images(folder_path):\n",
    "    images = os.listdir(folder_path)\n",
    "    image_list = []\n",
    "    \n",
    "    for image_name in images:\n",
    "        if image_name.endswith('.jpg'):\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "            \n",
    "            # Read image using PIL (Python Imaging Library)\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Convert image to numpy array\n",
    "            image_data = plt.imread(image_path)\n",
    "            \n",
    "            # Append image and file name to a list\n",
    "            image_list.append({'Image': image_data, 'File Name': image_name})\n",
    "    \n",
    "    # Create a DataFrame from the image data\n",
    "    df_images = pd.DataFrame(image_list)\n",
    "    \n",
    "    return df_images\n",
    "\n",
    "# Create DataFrame with images\n",
    "df_images = display_images(chair_folder)\n",
    "\n",
    "# Display the DataFrame with updated 'File Name'\n",
    "print(\"DataFrame with updated 'File Name':\")\n",
    "print(df_images.head)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_images.to_csv('updated_file_names.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4d5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4456f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/amin/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Compute a feature score for each image (using an example of a feature dimension)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# There are a maximum of 1000 dimensions which can be modelled.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m feature_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m\n\u001b[0;32m---> 47\u001b[0m feature_scores \u001b[38;5;241m=\u001b[39m \u001b[43mall_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assuming the feature dimension is 2048\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Normalize the feature scores to range between 0 and 1\u001b[39;00m\n\u001b[1;32m     50\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to preprocess images for ResNet\n",
    "def preprocess_resnet(image):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# Load ResNet model\n",
    "resnet = resnet50(pretrained=True)\n",
    "resnet.eval()\n",
    "\n",
    "# Function to extract ResNet features\n",
    "def extract_features(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        features = resnet(image_tensor)\n",
    "    return features[0]  # Get the feature vector, remove the batch dimension\n",
    "\n",
    "# Path to the folder containing images\n",
    "images_folder = Path('processed_images')\n",
    "\n",
    "# List to store ResNet features and image paths\n",
    "all_features = []\n",
    "image_paths = []\n",
    "\n",
    "# Process the first 500 images\n",
    "num_images = 500\n",
    "batch_size = 50\n",
    "\n",
    "for i, image_file in enumerate(images_folder.glob(\"*.jpg\")):\n",
    "    if i >= num_images:\n",
    "        break\n",
    "    image_paths.append(image_file.name)\n",
    "    image = Image.open(image_file)\n",
    "    image_tensor = preprocess_resnet(image)\n",
    "    features = extract_features(image_tensor)\n",
    "    all_features.append(features.numpy())  # Convert to numpy array and append\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "# Compute a feature score for each image (using an example of a feature dimension)\n",
    "# There are a maximum of 1000 dimensions which can be modelled.\n",
    "feature_dim = 75\n",
    "feature_scores = all_features[:, feature_dim]  # Assuming the feature dimension is 2048\n",
    "\n",
    "# Normalize the feature scores to range between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "feature_scores_normalized = scaler.fit_transform(feature_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Map the normalized scores to a colormap\n",
    "cmap = plt.get_cmap('viridis')  # You can change 'viridis' to any other colormap\n",
    "colors = [cmap(score) for score in feature_scores_normalized]\n",
    "\n",
    "# Reduce dimensions using UMAP\n",
    "reducer = UMAP(n_components=3)  # Set n_components to 3 for 3D visualization\n",
    "embedding = reducer.fit_transform(all_features)\n",
    "\n",
    "# Create DataFrame for the reduced dimensions\n",
    "umap_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "\n",
    "# Add image paths and feature scores to the DataFrame\n",
    "umap_df['image_path'] = image_paths\n",
    "umap_df['feature_score'] = feature_scores_normalized\n",
    "\n",
    "# Create an interactive 3D plot with plotly express\n",
    "fig = px.scatter_3d(umap_df, x='UMAP1', y='UMAP2', z='UMAP3', color='feature_score',\n",
    "                    title=\"UMAP Visualization with Feature Scores in 3D\",\n",
    "                    color_continuous_scale='viridis', hover_data=['image_path', 'feature_score'])\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.8, color=colors))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05825099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to display image on a plot\n",
    "def display_image(image_data, ax):\n",
    "    ax.imshow(image_data)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Function to update the plot\n",
    "def update_plot(b):\n",
    "    # Find the images in the DataFrame based on the file names entered in text_box1 and text_box2\n",
    "    selected_image1 = df_images[df_images['File Name'] == text_box1.value]['Image'].values\n",
    "    selected_image2 = df_images[df_images['File Name'] == text_box2.value]['Image'].values\n",
    "    \n",
    "    if len(selected_image1) > 0 and len(selected_image2) > 0:\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        \n",
    "        display_image(selected_image1[0], ax1)\n",
    "        ax1.set_title(text_box1.value, fontsize=10)\n",
    "        \n",
    "        display_image(selected_image2[0], ax2)\n",
    "        ax2.set_title(text_box2.value, fontsize=10)\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "    else:\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            print('One or more images not found.')\n",
    "\n",
    "# Create initial empty plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Create initial empty images\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create text boxes for image paths\n",
    "text_box1 = widgets.Text(description='Image 1:', value='', continuous_update=False)\n",
    "text_box2 = widgets.Text(description='Image 2:', value='', continuous_update=False)\n",
    "\n",
    "# Create update button\n",
    "button = widgets.Button(description='Update Plots')\n",
    "\n",
    "# Create output widget\n",
    "out = widgets.Output()\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.HBox([text_box1, text_box2, button]))\n",
    "display(out)\n",
    "\n",
    "# Define button click event handler\n",
    "button.on_click(update_plot)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
